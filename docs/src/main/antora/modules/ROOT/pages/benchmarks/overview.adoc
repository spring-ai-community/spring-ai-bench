= Benchmark Overview
:page-title: Benchmark Overview
:toc: left
:tabsize: 2
:sectnums:

Spring AI Bench benchmarks are designed to evaluate AI agents on real-world Java software engineering tasks.

== What Makes a Good Benchmark?

Effective benchmarks for AI developer agents should:

* **Reflect Real Workflows** - Mirror actual developer tasks and processes
* **Use Representative Codebases** - Work with typical enterprise Java projects
* **Measure Practical Skills** - Test abilities that matter in production
* **Avoid Overfitting** - Resist gaming through memorization
* **Enable Custom Evaluation** - Work on your own codebases

== Current Implementation Status

=== âœ… Available Now

**hello-world Track**: Infrastructure validation and basic agent testing
* File creation and basic I/O operations
* Multi-agent comparison (deterministic vs AI-powered)
* Foundation for future track development

=== ðŸš§ Planned Benchmark Categories

These represent the enterprise Java workflows that will differentiate Spring AI Bench from Python-focused benchmarks:

==== Coding Tasks

Software development activities that agents should be able to perform:

* **Bug Fixing** - Diagnose and fix failing tests or runtime issues
* **Feature Implementation** - Add new functionality based on specifications
* **Refactoring** - Improve code structure while preserving behavior
* **API Migration** - Update code to use newer APIs or frameworks

==== Project Management

Higher-level tasks that demonstrate planning and coordination:

* **Issue Triage** - Analyze bug reports and feature requests
* **PR Review** - Evaluate code changes for quality and correctness
* **Test Coverage** - Identify and write tests for uncovered code
* **Documentation** - Generate or update technical documentation

==== Version Upgrade

Complex tasks involving dependency and framework management:

* **Dependency Upgrades** - Update libraries while fixing breaking changes
* **Framework Migration** - Move between framework versions (e.g., Spring Boot 2â†’3)
* **Java Version Upgrade** - Update codebases to newer Java versions
* **Build System Migration** - Convert between build tools (e.g., Mavenâ†’Gradle)

== Benchmark Structure

=== YAML Specification

Each benchmark is defined by a YAML file:

[source,yaml]
----
# hello-world.yaml - Current working example
id: hello-world
category: coding
version: 0.5

repo:
  owner: example-org
  name: example-repo
  ref: main

agent:
  kind: hello-world-ai
  provider: claude
  autoApprove: true
  prompt: |
    Create a hello world file with the specified content.

success:
  cmd: ls hello.txt

timeoutSec: 600
----

=== Component Breakdown

==== Repository Specification

* **Owner/Name** - GitHub repository coordinates
* **Ref** - Specific commit, branch, or tag to checkout
* **Subpath** - Optional subdirectory focus (for monorepos)

==== Agent Configuration

* **Kind** - Agent type (currently: `hello-world`, `hello-world-ai`)
  * `hello-world` - Deterministic mock agent for testing
  * `hello-world-ai` - AI-powered agent via Spring AI Agents integration
* **Provider** - AI provider for hello-world-ai (`claude`, `gemini`)
* **Prompt** - Natural language task description
* **Auto-approve** - Whether to bypass human confirmation prompts

==== Success Criteria

* **Command** - Shell command to verify success
* **Exit Code** - Expected exit code (usually 0)
* **Timeout** - Maximum time for verification
* **File Checks** - Optional file existence/content verification

== Evaluation Metrics

=== Primary Metrics

* **Success Rate** - Percentage of benchmarks completed successfully
* **Time to Completion** - Duration from start to successful completion
* **Attempt Efficiency** - Ratio of successful attempts to total attempts

=== Secondary Metrics

* **Code Quality** - Style, maintainability, and best practices
* **Test Coverage** - Percentage of code covered by tests
* **Security Compliance** - Absence of security vulnerabilities
* **Resource Usage** - CPU, memory, and network consumption

== Benchmark Design Principles

=== Realistic Scenarios

Benchmarks should represent tasks that developers actually perform:

* Use **real codebases** with typical complexity
* Include **context and constraints** found in production
* Require **multi-step reasoning** and planning
* Test **tool usage** and command-line interaction

=== Evaluation Robustness

Prevent gaming and ensure fair evaluation:

* Use **multiple repositories** for each task type
* Include **negative cases** that should fail
* Test **edge cases** and error conditions
* Rotate **repository versions** to prevent memorization

=== Scalability

Design for efficient execution and maintenance:

* Support **parallel execution** of multiple benchmarks
* Enable **batch processing** for large benchmark suites
* Provide **filtering and tagging** for selective execution
* Include **cleanup and isolation** to prevent interference

== Multi-Agent Benchmarking

Spring AI Bench supports comparative testing between different agent types, enabling performance analysis across deterministic and AI-powered agents.

=== Comparative Testing

Multi-agent benchmarks run the same task across different implementations:

* **Deterministic Implementation** - `hello-world` agent provides fast, predictable baseline
* **AI-Powered Implementation** - `hello-world-ai` agent uses spring-ai-agents framework
  - Claude provider for complex reasoning tasks
  - Gemini provider for balanced speed/capability
* **Direct CLI Agents** - `claude-code` and `gemini` for direct integration

=== Performance Metrics

Key metrics for multi-agent comparison:

* **Execution Time** - Duration from start to completion
* **Success Rate** - Percentage of successful task completions
* **Performance Ratio** - Relative speed compared to baseline
* **Accuracy** - Quality and correctness of output

=== Integration with Spring AI Agents

The `hello-world-ai` agent type provides end-to-end integration with the Spring AI Agents framework:

* **JBang Launcher** - Seamless execution via JBang command-line tool
* **Provider Selection** - Choose between Claude, Gemini, or other AI providers
* **Local Testing** - Full integration testing without external dependencies
* **Report Generation** - Comprehensive HTML reports with agent metadata

== Next Steps

* xref:benchmarks/running-benchmarks.adoc[Running Benchmarks] - Execute existing benchmarks
* xref:benchmarks/writing-benchmarks.adoc[Writing Benchmarks] - Create custom benchmarks
* xref:agents/claude-code.adoc[Agent Integration] - Set up AI agents for benchmarking