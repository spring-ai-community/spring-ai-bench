= Spring AI Bench
:page-title: Spring AI Bench
:toc: left
:tabsize: 2
:sectnums:

Open benchmarking suite for Java-centric AI developer agents.

== What & Why

**The Problem**: Existing benchmarks (SWE-bench) measure:

- **Yesterday's agents**: Academic scaffolding (https://github.com/SWE-agent/SWE-agent[SWE-agent]: ~11,300 lines)
- **Yesterday's tasks**: Static 2023 Python patches
- **One architecture**: Can't measure Claude, Gemini, Amazon Q (the agents teams actually use)

**Spring AI Bench measures**:

- **Modern agents**: Claude, Gemini, Amazon Q, Ampâ€”any agent via `AgentModel` abstraction
- **Enterprise Java workflows**: Issue triage, PR review, coverage, complianceâ€”not just patches
- **Your code**: Run benchmarks on YOUR repos, measure YOUR scenarios

**If agents have evolved, benchmarks must evolve too.**

== Why SWE-bench Falls Short: At a Glance

[cols="2,3,3"]
|===
|Issue |SWE-bench Problem |Spring AI Bench Solution

|**Scope**
|Patch loops only (bug fixes)
|Full dev lifecycle (triage, PR review, coverage, compliance, deps, migrations)

|**Contamination**
|60%+ Verified â†’ 19% Live (same agent, 3x drop on fresh tasks)
|Golden benchmark set (curated) + run same benchmarks on YOUR repos

|**Language Bias**
|Python-only: ~75% scores; Java: ~7-10% (order of magnitude gap from training data bias)
|Java-first with Maven, Spring Boot, complex dependency trees

|**Reproducibility**
|No disclosure required; can't verify leaderboard claims
|One-click Docker containers + open scaffolding + transparent judges
|===

**Real code. Real runs. Real Java.** Not static Python patches from 2023.

**â†’ See detailed evidence & analysis below** (Section 6)

== Spring AI Bench: What You Get

**Built for Reproducibility** (https://betterbench.stanford.edu/[BetterBench] as north star):

**Multi-Agent Support**:

- âœ… **Any agent**: Claude Code, Gemini CLI, Amazon Q Developer, Amp, Codex, custom implementations
- âœ… **Not locked in**: AgentModel abstraction lets you measure the agents your team uses
- âœ… **Real-world**: Measure production agents, not academic research artifacts

**Enterprise Java Workflows**:

- âœ… **Full lifecycle**: Issue triage â†’ PR review â†’ coverage uplift â†’ compliance validation â†’ dependency upgrades
- âœ… **Real complexity**: Maven, JDK versions, Spring Boot, complex dependency trees
- âœ… **Your repos**: Not static 2023 GitHub dataâ€”run on YOUR code

**Modern Standards**:

- âœ… **One-click reproducible**: Docker containers with pre-configured environments (no "works on my machine")
- âœ… **Multi-dimensional**: Success rate + cost + speed + reliability + quality (not just pass/fail)
- âœ… **Open & transparent**: GitHub repo, Apache 2.0 license, reproducible evaluation code
- âœ… **Judge framework**: Sophisticated verification beyond "tests pass" (deterministic + AI-powered judges)

**Reproducibility First**:

- âœ… **One-click setup**: Docker containers, pre-configured environments
- âœ… **Open methodology**: Published evaluation code, transparent scoring
- âœ… **Golden set + YOUR repos**: Standardized comparison + real-world validation
- âœ… **BetterBench inspiration**: Following Stanford's quality framework principles (starting with reproducibility)

**Run benchmarks on YOUR code. Measure YOUR scenarios. See which agent wins for YOU.**

== How Spring AI Bench Compares

[cols="2,3,3"]
|===
|Dimension |SWE-bench (2023-2024) |Spring AI Bench (2025)

|*Scope*
|Patch loops for bug fixing
|Full enterprise development lifecycle

|*Language*
|Python-only (12 repos)
|Java-first (extensible to Kotlin, Groovy)

|*Agent Support*
|One architecture (SWE-agent: ~11,300 lines)
|Any agent (Claude, Gemini, Q, custom via AgentModel)

|*Reproducibility*
|No disclosure required (can't verify claims)
|One-click Docker + open scaffolding + transparent judges

|*Agent Paradigm*
|Built for 2024 patch-loop agents
|Built for 2025 declarative goal agents

|*Standards*
|Pre-BetterBench
|Following BetterBench principles (reproducibility first)
|===

**Historical value**: SWE-bench pioneered agent evaluation

**Modern needs**: Spring AI Bench measures what enterprise teams care about

== Architecture Overview

[source]
----
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agent Types   â”‚    â”‚  Execution Core  â”‚    â”‚   Sandboxes     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Claude Code  â”‚â”€â”€â”€â”€â”‚ BenchHarness     â”‚â”€â”€â”€â”€â”‚LocalSandbox     â”‚
â”‚ âœ… Gemini       â”‚    â”‚ AgentRunner      â”‚    â”‚DockerSandbox    â”‚
â”‚ âœ… Amazon Q     â”‚    â”‚ SpecLoader       â”‚    â”‚CloudSandbox     â”‚
â”‚ âœ… Amp          â”‚    â”‚ ReportGenerator  â”‚    â”‚   (Future)      â”‚
â”‚ âœ… Custom       â”‚    â”‚ Judge Framework  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
----

**Skills, not just tools**: Benchmarks encode _skills_ (context + actions + success criteria). Tools matter, but the plan and verification criteria matter just as much. Where possible, we align with https://modelcontextprotocol.io/[Model Context Protocol (MCP)] to keep tool use portable across agents.

== Benchmark Tracks: The Vision

**âœ… Available Now**:

- **hello-world**: File creation and infrastructure validation

**ðŸš§ In Active Development**:

- **Test Coverage Uplift**: Generate tests to achieve coverage thresholds while keeping builds green
- **Issue Analysis & Labeling**: Automated triage with domain-specific classification
- **Pull Request Review**: Comprehensive PR analysis with risk assessment and policy compliance
- **Static Analysis Remediation**: Fix checkstyle violations while preserving functionality

**ðŸ“‹ Future Roadmap**:

Integration testing, dependency upgrades, API migrations, compliance validation, performance optimization, documentation generation

**This breadth sets Spring AI Bench apart**â€”measuring the full spectrum of enterprise Java development.

== Next Steps

Ready to get started?

- **Try it**: xref:getting-started.adoc[Getting Started Guide] - Quick setup and first benchmark
- **Understand it**: xref:architecture.adoc[Architecture Overview] - System design and components
- **Verify it**: xref:betterbench-compliance.adoc[BetterBench Alignment] - Our commitment to quality standards
- **Integrate it**: xref:agents/claude-code.adoc[Agent Integration] - Connect Claude, Gemini, Amazon Q, or custom agents

**Have questions?** See detailed evidence and analysis below.

'''

== The Evidence: Why SWE-bench Falls Short

_This section provides detailed evidence for the claims in the summary tables above._

=== The Paradigm Shift: From Scaffolding to Declarative Agents

The software development agent landscape has fundamentally changed:

**2023-2024: The Scaffolding Era**

- Agents required complex client-side engineering (https://github.com/SWE-agent/SWE-agent[SWE-agent]: ~11,300 lines of code):
  * https://github.com/SWE-agent/SWE-agent/blob/main/sweagent/agent/agents.py#L413[Multi-step loops] (`while not step_output.done`)
  * https://github.com/SWE-agent/SWE-agent/blob/main/sweagent/agent/agents.py#L60-L146[Prompt orchestration] (Jinja2 templates for system, instance, next-step)
  * https://github.com/SWE-agent/SWE-agent/blob/main/sweagent/agent/agents.py#L189-L369[Error recovery] (retry loops, exception handling)
- Benchmarks designed for patch-based workflows: edit â†’ test â†’ repeat
- https://www.swebench.com/[SWE-bench] pioneered agent evaluation for code fixes

**2025: The Declarative Era**

- Reasoning models internalize planning (GPT-4o, Claude Opus 4, Gemini 2.0)
- Model Context Protocol (MCP) standardizes tool and context management
- Agents accept declarative goals: "Raise coverage to 80%" vs procedural steps

**https://spring-ai-community.github.io/spring-ai-agents[Spring AI Agents] embodies this shift**:

> "The shift: from **imperative** (code every workflow step) to **declarative** (describe the goal and let the model plan the steps)."

SWE-bench measured yesterday's agents (academic SWE-agent scaffolding) with yesterday's tasks (static 2023 Python patches).

Spring AI Bench measures **modern agents** (Claude, Gemini, Amazon Qâ€”the ones enterprises actually use) on **enterprise Java workflows**.

=== Contamination & Overfitting: The Evidence

**First, establish the baseline**: Recent agents achieve impressive scores on SWE-bench's static datasets:

- **SWE-bench Verified** (static 2023 Python issues): Top agents exceed **60%** resolved rate
- **Best published result**: Claude Opus 4.1 at **~75%** (Anthropic 2025)
- **Community consensus**: SWE-bench Verified is the "gold standard" for agent evaluation

**But then, something doesn't add up:**

When the **exact same agents** run on fresh, unseen issues (SWE-bench Live, 2025), performance collapses:

**SWE-bench Live Results** (https://arxiv.org/abs/2505.23419[arxiv:2505.23419]):

- OpenHands + Claude 3.7 Sonnet on **Verified** (static 2023 data): **~43%**
- OpenHands + Claude 3.7 Sonnet on **Live** (new 2025 issues): **~19%**
- **Same agent, same settings, ~2x performance drop**

The paper states: _"Recent state-of-the-art agents and models report a resolved rate exceeding 60% on the SWE-bench Verified subset. In contrast, the highest resolved rate on SWE-bench-Live is only 19%."_

**This fails a common-sense test**: Imagine a student preparing for a standardized exam (SAT, A-levels, Gaokao):

- **Practice tests**: Scores ~75% (top tier)
- **Real exam**: Scores ~19% (bottom half)
- **Same student, same preparation, ~2x score collapse**

What's the logical explanation?

1. **The practice tests leaked the answers** - questions and solutions were in study materials
2. **The student memorized patterns, not concepts** - optimized for specific question types
3. **The practice tests weren't representative** - easier or structurally different from real exam

No credible educator would accept this as measuring genuine knowledge. **The same logic applies here**: The evidence points to **overfitting and contamination**, not genuine capability.

The SWE-bench Live paper itself states: _"This raises concerns about potential overfitting to SWE-bench."_

**The mini-swe-agent paradox**: https://github.com/SWE-agent/mini-swe-agent[mini-swe-agent] claims to achieve 68% on SWE-bench Verified with just ~100 lines of code (vs ~11,300 for full SWE-agent). If a trivial bash-only agent with linear history matches complex scaffolding performance, this suggests:

1. **Either**: The benchmark is too easy (agents memorized solutions)
2. **Or**: Complex scaffolding was never needed (the problem was simpler than claimed)

**Tellingly**: mini-swe-agent has not published SWE-bench-Live results. We can infer the likely outcome from the pattern: simple approaches work on static data, fail on fresh tasks.

**SWE-bench+ Analysis** (https://openreview.net/forum?id=pwIGnH2LHJ[OpenReview]):

- ~33% of "successful" patches had solution leakage (answer in issue text)
- ~31% passed due to weak test suites (not truly fixed)
- GPT-4 + SWE-agent: ~12% â†’ ~4% after filtering
- **Benchmark has structural evaluation weaknesses**

**SWE-bench Illusion** (https://arxiv.org/abs/2506.12286[arxiv:2506.12286]):

- "When State-of-the-Art LLMs Remember Instead of Reason"
- Documents contamination across multiple coding benchmarks

**https://betterbench.stanford.edu/[BetterBench] Recommendation**: Dynamic task generation, live updates, contamination resistance

**Spring AI Bench Approach**: Run on YOUR repos with fresh goals (not static 2023 GitHub data)

=== Language Bias: The Python-Only Problem

**SWE-bench only measures Python**. All 2,294 tasks in the original dataset come from 12 Python repositories. This creates a fundamental problem for **enterprise Java teams**.

**Why this matters**: When researchers finally tested agents on other languages (Java, TypeScript, Go), the results were shocking:

**The Numbers** (exact citations):

[cols="2,1,1,3"]
|===
|Benchmark |Language |Top Score |Citation

|SWE-bench Verified
|Python
|~75% (Claude Opus 4.1)
|https://www.anthropic.com/news/claude-opus-4-1[Anthropic 2025]

|SWE-bench-Java Verified
|Java
|~10% (DeepSeek-V2, 9/91 tasks)
|https://arxiv.org/abs/2408.14354[arxiv:2408.14354]

|SWE-bench-Java Verified
|Java
|~7% (GPT-4o, 6/91 tasks)
|https://arxiv.org/abs/2408.14354[arxiv:2408.14354]

|SWE-PolyBench
|Python
|~24%
|https://arxiv.org/abs/2504.08703[arxiv:2504.08703]

|SWE-PolyBench
|TypeScript
|~5%
|https://arxiv.org/abs/2504.08703[arxiv:2504.08703]
|===

**Order of magnitude gap**: Python ~75% vs Java ~7-10%

**Why?** The SWE-PolyBench paper investigated this and found: _"This performance distribution cannot be explained by complexity metrics alone... pass rates stem from... language-specific factors that likely reflect **the distribution of programming languages and structural patterns in LLMs' pretraining data**."_

**In plain terms**: Models were trained predominantly on Python code. When tested on Java, TypeScript, or Go, they struggleâ€”not because these languages are harder, but because the models have seen far less training data in these languages.

This aligns with real-world experience: developers using AI coding tools (https://ampcode.com/how-i-use-amp[like Amp]) report that agents work better on Python than on enterprise Java workflows. The paradigm shift to declarative agents doesn't eliminate the **training data bias problem**.

**Enterprise reality**: Most critical systems use Java, Kotlin, C#, Go (polyglot stacks) - precisely the languages where current benchmarks show agents struggling

**Spring AI Bench**: Java-first design handles Maven, JDK, Spring Boot complexity from day one

=== Scaffolding Opacity: The Reproducibility Problem

**Modern benchmark standards require reproducibility** (https://betterbench.stanford.edu/[BetterBench criteria]).

**The problem**: SWE-bench doesn't require submissions to disclose their architecture or provide reproduction scripts. This isn't Anthropic's faultâ€”**it's a benchmark design flaw**.

**What we observe**:

- https://www.anthropic.com/research/swe-bench-sonnet[Anthropic's blog] describes "agent system": scaffolding, prompt formatting, iteration, error recovery
- Blog doesn't disclose exact prompts, retry strategies, selection logic
- Community projects (https://github.com/augmentcode/augment-swebench-agent[augmentcode/augment-swebench-agent]) had to reverse-engineer planning tools
- Same model, different scaffolds = different scores (but we can't compare because architectures aren't disclosed)

**BetterBench verdict**: A benchmark that doesn't require reproducible submissions fails a fundamental quality criterion.

**But there's a deeper issue**: Why should we measure only one agent architecture?

**SWE-bench measures one academic agent** (SWE-agent: ~11,300 lines, developed part-time by researchers). Meanwhile:

- **Google, Amazon, OpenAI, Anthropic, Microsoft**: Teams of dozens working full-time on production agents
- **Startups** (Poolside, Magic, Cognition): Venture-funded teams building commercial agents
- **The market reality**: Enterprise teams don't use SWE-agentâ€”they use Claude, Gemini, GitHub Copilot, Cursor, etc.

**Spring AI Agents embraces this reality**: Our `AgentModel` abstraction lets you measure ANY agent:

- Claude Code (Anthropic's production agent)
- Gemini with code execution
- Amazon Q Developer
- Amp, Codex, or custom implementations

**Spring AI Bench follows suit**: Measure the agents teams actually use, not just academic research artifacts. That's what BetterBench calls forâ€”benchmarks aligned with real-world usage.

**And we provide one-click reproducibility**: Every benchmark runs in a Docker container with a pre-configured environment. No "works on my machine" excuses. Clone, run, verify. That's the BetterBench standard.

=== BetterBench Alignment Roadmap

For our roadmap toward alignment with Stanford's 46-criteria framework, see xref:betterbench-compliance.adoc[BetterBench Alignment Roadmap].

**Current focus areas**:

- **Design**: User personas, domain experts, multi-dimensional metrics, judge framework
- **Implementation**: Open source, one-click Docker reproducibility, multi-provider support
- **Documentation**: Architecture docs, getting started, limitations, Apache 2.0 license
- **Maintenance**: Active development, GitHub feedback channels, maintainer team

**Note**: This is a self-assessment roadmap, not a validated BetterBench certification. We're working toward alignment with these quality standards.

== References

=== BetterBench

* **Reuel et al. (2024)**, "BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices," NeurIPS 2024 Datasets & Benchmarks Track
* **Paper**: https://arxiv.org/abs/2411.12990
* **Website**: https://betterbench.stanford.edu/
* **Framework**: 46 criteria across design, implementation, documentation, and maintenance stages

=== SWE-bench Research

**Original SWE-bench**:

* **Jimenez et al. (2024)**, "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?" ICLR 2024
* **Paper**: https://arxiv.org/abs/2310.06770
* **Website**: https://www.swebench.com/

**SWE-bench-Java**:

* **Dakhel et al. (2024)**, "SWE-bench-Java: A Multi-Language Benchmark for Issue Resolving"
* **Paper**: https://arxiv.org/abs/2408.14354
* **Key Results**: GPT-4o ~7% (6/91), DeepSeek-V2 ~10% (9/91) on Verified Java tasks

**SWE-bench-Live**:

* **Jimenez et al. (2025)**, "SWE-bench Goes Live!"
* **Paper**: https://arxiv.org/abs/2505.23419
* **Key Finding**: Best agent ~19% on Live vs 60%+ on Verified (same agent, 3x drop)

**SWE-PolyBench**:

* **AWS (2025)**, "SWE-PolyBench: A Multilingual Benchmark for Code Agents"
* **Paper**: https://arxiv.org/abs/2504.08703
* **Key Results**: Python ~24%, TypeScript ~5%, Java moderate performance

**SWE-bench-Illusion**:

* **Authors (2025)**, "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason"
* **Paper**: https://arxiv.org/abs/2506.12286
* **Focus**: Contamination analysis across coding benchmarks

**SWE-bench+**:

* **Authors (2024)**, "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
* **Paper**: https://openreview.net/forum?id=pwIGnH2LHJ
* **Key Findings**: ~33% solution leakage, ~31% weak test suites, GPT-4 ~12%â†’~4% after filtering

=== Model Performance

**Anthropic Claude Opus 4.1**:

* **Anthropic (2025)**, "Claude Opus 4.1"
* **Blog**: https://www.anthropic.com/news/claude-opus-4-1
* **SWE-bench Verified**: ~75% (Python)

**Anthropic SWE-bench Engineering**:

* **Anthropic (2024)**, "Raising the bar on SWE-bench Verified with Claude 3.5 Sonnet"
* **Blog**: https://www.anthropic.com/research/swe-bench-sonnet
* **Description**: Agent system engineering with scaffolding, prompt formatting, iteration, error recovery

=== Community Analysis

**Runloop Blog**:

* **Runloop (2024)**, "SWE-bench Deep Dive: Unmasking the Limitations"
* **Blog**: https://www.runloop.ai/blog/swe-bench-deep-dive

**Community Implementations**:

* **augmentcode/augment-swebench-agent**: Community fork based on Anthropic's architecture
* **GitHub**: https://github.com/augmentcode/augment-swebench-agent

=== Related Frameworks

**Spring AI Agents**:

* **Spring AI Community**, "Spring AI Agents: The pragmatic integration layer for autonomous agents in Java enterprise development"
* **Documentation**: https://spring-ai-community.github.io/spring-ai-agents
* **GitHub**: https://github.com/spring-ai-community/spring-ai-agents
* **Key Concept**: Paradigm shift from imperative (code every step) to declarative (describe the goal)
